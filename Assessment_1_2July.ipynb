pip install pdfplumber

import pdfplumber
import tensorflow as tf
import keras

text = ""
with pdfplumber.open("/Workspace/Users/dbuser11@meteoros.ai/gen_ai_sample_pdf_assignment1.pdf") as pdf:
    for page in pdf.pages:
        text += page.extract_text() + "\n"

print(text)



import spacy
# !python -m spacy download en_core_web_lg

## Tokeinizing entire corpus into words
nlp = spacy.load("en_core_web_lg")
doc = nlp(text)
tokens = [token.text for token in doc]
print(tokens)



# !pip install gensim
# !pip install --upgrade numpy
!pip install --upgrade numpy gensim


corpus = [tokens] 

dbutils.fs.mkdirs("dbfs:/Assignment_1/word_embeddings")


from gensim.models import Word2Vec

embedding_dir = "Assignment_1/word_embeddings"  # DBFS path

for dim in [8, 16, 32]:
    model = Word2Vec(corpus, vector_size=dim, window=2, min_count=1, workers=1)
    
    file_path = f"/dbfs/{embedding_dir}/embedding_{dim}d.txt"
    
    with open(file_path, "w") as f:
        for word in model.wv.index_to_key:
            vector = " ".join(map(str, model.wv[word]))
            f.write(f"{word} {vector}\n")

    print(f"Saved {dim}d embeddings to {file_path}")


# from gensim.models import Word2Vec

# User input
input_word = input("Enter a word to search in embeddings: ").strip().lower()

# DBFS path
embedding_dir = "Assignment_1/word_embeddings"
dims = [8, 16, 32]
found = False

for dim in dims:
    file_path = f"/dbfs/{embedding_dir}/embedding_{dim}d.txt"

    # Read existing words from the file
    try:
        with open(file_path, "r") as f:
            lines = f.readlines()
    except FileNotFoundError:
        lines = []

    # Safely extract first word from each line (ignoring blanks)
    words_in_file = {line.split()[0] for line in lines if line.strip()}

    if input_word in words_in_file:
        print(f"Found '{input_word}' in {dim}d embeddings.")
        found = True
    else:
        print(f" '{input_word}' not found in {dim}d â€” generating embedding...")
        
        # Train a Word2Vec model on just this word
        dummy_corpus = [[input_word]]
        model = Word2Vec(dummy_corpus, vector_size=dim, window=1, min_count=1, workers=1)

        vector = " ".join(map(str, model.wv[input_word]))

        # Append new embedding to file
        # Prepare new line
    new_line = f"{input_word} {vector}\n"

    # Combine old + new
    all_lines = lines + [new_line]

    # Overwrite the entire file
    with open(file_path, "w") as f:
        f.writelines(all_lines)
        print(f" Appended '{input_word}' to {dim}d file.")

if not found:
    print(f"\n '{input_word}' was not found and has now been embedded and saved.")


dbutils.fs.cp(
    "dbfs:/Assignment_1/word_embeddings/embedding_8d.txt",
    "dbfs:/FileStore/embedding_8d.txt"
)
dbutils.fs.cp(
    "dbfs:/Assignment_1/word_embeddings/embedding_16d.txt",
    "dbfs:/FileStore/embedding_16d.txt"
)
dbutils.fs.cp(
    "dbfs:/Assignment_1/word_embeddings/embedding_32d.txt",
    "dbfs:/FileStore/embedding_32d.txt"
)
